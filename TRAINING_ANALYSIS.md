# TRAINING ANALYSIS & IMPROVEMENTS

## üìä PH√ÇN T√çCH V·∫§N ƒê·ªÄ

### K·∫øt Qu·∫£ Training Tr∆∞·ªõc ƒê√¢y:

| Model | Terminal 1 Accuracy | Terminal 2,3 Accuracy | V·∫•n ƒê·ªÅ |
|-------|--------------------|-----------------------|---------|
| **CNN Simple** | 95.59% ‚úÖ | 93.48% ‚úÖ | OK - Ch√™nh l·ªách nh·ªè do random seed |
| **CNN Deep** | N/A | N/A | Ch∆∞a train trong Terminal 1 |
| **LSTM** | N/A | N/A | Ch∆∞a train trong Terminal 1 |
| **CNN-LSTM** | 85.21% ‚úÖ | 51.20% ‚ùå | C√≥ v·∫•n ƒë·ªÅ l·ªõn |
| **Depthwise CNN** | 60.06% ‚ùå | 42.21% ‚ùå | Model architecture y·∫øu |
| **CNN Attention** | 86.39% ‚úÖ | 86.43% ‚úÖ | OK - K·∫øt qu·∫£ g·∫ßn gi·ªëng nhau |

### Nguy√™n Nh√¢n:

1. **Depthwise CNN c√≥ accuracy th·∫•p (60% v√† 42%)**:
   - Thi·∫øu BatchNormalization layers
   - Ch·ªâ c√≥ 2 blocks, qu√° ƒë∆°n gi·∫£n
   - Dense layer cu·ªëi ch·ªâ c√≥ 64 units

2. **CNN-LSTM c√≥ s·ª± ch√™nh l·ªách l·ªõn (85% vs 51%)**:
   - C√≥ th·ªÉ do random initialization kh√°c nhau
   - LSTM r·∫•t nh·∫°y c·∫£m v·ªõi initial weights
   - C·∫ßn train nhi·ªÅu l·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o stability

3. **Terminal 1 vs Terminal 2,3**:
   - Code gi·ªëng nhau 100%
   - Ch√™nh l·ªách do random seed v√† th·ª© t·ª± training
   - Kh√¥ng ph·∫£i l·ªói code

---

## ‚úÖ GI·∫¢I PH√ÅP ƒê√É √ÅP D·ª§NG

### 1. C·∫£i Thi·ªán Depthwise Separable CNN:

**Tr∆∞·ªõc:**
```python
# Ch·ªâ 2 blocks, kh√¥ng c√≥ BatchNorm
layers.DepthwiseConv1D(kernel_size=5, ...)
layers.Conv1D(32, kernel_size=1, ...)
layers.MaxPooling1D(pool_size=2)
layers.Dropout(0.2)
```

**Sau:**
```python
# 3 blocks, c√≥ BatchNorm
layers.DepthwiseConv1D(kernel_size=5, ...)
layers.BatchNormalization()  # ‚Üê TH√äM
layers.Conv1D(32, kernel_size=1, ...)
layers.BatchNormalization()  # ‚Üê TH√äM
layers.MaxPooling1D(pool_size=2)
layers.Dropout(0.2)

# Block 3 m·ªõi
layers.DepthwiseConv1D(kernel_size=3, ...)
layers.BatchNormalization()
layers.Conv1D(128, kernel_size=1, ...)
layers.BatchNormalization()

# Dense layer l·ªõn h∆°n
layers.Dense(128, activation='relu')  # 64 ‚Üí 128
layers.Dropout(0.4)  # 0.3 ‚Üí 0.4
```

### 2. ƒê·∫£m B·∫£o Train ƒê·ªß 6 Models:

**File `train_all_models.py` ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t:**
```python
model_configs = [
    {'name': 'cnn_simple', 'description': 'Baseline CNN Simple'},
    {'name': 'cnn_deep', 'description': 'Deep CNN'},           # ‚Üê TH√äM
    {'name': 'lstm', 'description': 'LSTM Model'},             # ‚Üê TH√äM
    {'name': 'cnn_lstm', 'description': 'CNN-LSTM Hybrid'},
    {'name': 'depthwise_cnn', 'description': 'Depthwise Separable CNN'},
    {'name': 'cnn_attention', 'description': 'CNN with Attention'},
]
```

---

## üöÄ TRAINING M·ªöI

### Script: `final_training_all_6_models.py`

**ƒê·∫∑c ƒëi·ªÉm:**
- ‚úÖ Train ƒë·ªß 6 models
- ‚úÖ Depthwise CNN ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán
- ‚úÖ 50 epochs v·ªõi Early Stopping
- ‚úÖ ReduceLROnPlateau ƒë·ªÉ t·ªëi ∆∞u learning rate
- ‚úÖ L∆∞u k·∫øt qu·∫£ chi ti·∫øt

**K·ª≥ v·ªçng accuracy sau khi c·∫£i thi·ªán:**

| Model | Expected Accuracy | Model Size |
|-------|------------------|------------|
| CNN Simple | 95-96% | ~1.1 MB |
| CNN Deep | 96-97% | ~2.5 MB |
| LSTM | 93-95% | ~1.5 MB |
| CNN-LSTM | 96-97% | ~160 KB |
| **Depthwise CNN** | **92-95%** ‚¨ÜÔ∏è | ~50 KB |
| CNN Attention | 95-96% | ~120 KB |

---

## üìÅ FILES ƒê∆Ø·ª¢C T·∫†O

### Training Files:
- `final_training_all_6_models.py` - Script training ch√≠nh
- `final_training_output.txt` - Log ƒë·∫ßy ƒë·ªß
- `train_individual_models.py` - Train t·ª´ng model ri√™ng
- `launch_parallel_training.py` - Train song song
- `monitor_training.py` - Monitor ti·∫øn tr√¨nh

### Model Files:
- `models/har_model_cnn_simple.h5`
- `models/har_model_cnn_deep.h5`
- `models/har_model_lstm.h5`
- `models/har_model_cnn_lstm.h5`
- `models/har_model_depthwise_cnn.h5` ‚Üê Improved
- `models/har_model_cnn_attention.h5`

### Results Files:
- `models/training_results_comparison.json`
- `models/model_comparison_report.txt`
- `models/model_comparison_plots.png`

---

## üéØ K·∫æT LU·∫¨N

### V·∫•n ƒê·ªÅ Ch√≠nh:
1. ‚ùå Depthwise CNN architecture qu√° ƒë∆°n gi·∫£n
2. ‚ùå Thi·∫øu BatchNormalization
3. ‚ùå Ch∆∞a train ƒë·ªß 6 models

### Gi·∫£i Ph√°p:
1. ‚úÖ C·∫£i thi·ªán Depthwise CNN v·ªõi BatchNorm v√† th√™m 1 block
2. ‚úÖ TƒÉng s·ªë units trong Dense layer (64 ‚Üí 128)
3. ‚úÖ C·∫≠p nh·∫≠t train_all_models.py ƒë·ªÉ train ƒë·ªß 6 models
4. ‚úÖ T·∫°o script final_training_all_6_models.py

### K·∫øt Qu·∫£ Mong ƒê·ª£i:
- Depthwise CNN: 60% ‚Üí **92-95%** (tƒÉng ~35%)
- T·∫•t c·∫£ 6 models ƒë·ªÅu ƒë·∫°t accuracy > 90%
- Models nh·ªè g·ªçn, ph√π h·ª£p cho ESP32

---

## üìù NEXT STEPS

Sau khi training ho√†n t·∫•t (~15-20 ph√∫t):

1. **Ki·ªÉm tra k·∫øt qu·∫£:**
   ```bash
   python monitor_training.py
   cat models/model_comparison_report.txt
   ```

2. **Evaluate models:**
   ```bash
   python src/evaluate_all_models.py
   ```

3. **Deploy to ESP32:**
   ```bash
   python src/deploy_all_models.py
   ```

---

**Training Status:** üèÉ Running in Terminal 6
**Expected Completion:** ~15-20 minutes
**Output Log:** `final_training_output.txt`

