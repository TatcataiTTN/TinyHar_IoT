#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluation Script cho HAR Model
ÄÃ¡nh giÃ¡ model vÃ  táº¡o bÃ¡o cÃ¡o chi tiáº¿t
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow import keras
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Import cÃ¡c module khÃ¡c
from data_loader import load_uci_har_data
from preprocessing import preprocess_data, reshape_for_cnn

def evaluate_model(model_path='models/har_model_cnn_simple.h5'):
    """
    ÄÃ¡nh giÃ¡ model trÃªn test set
    
    Args:
        model_path: ÄÆ°á»ng dáº«n Ä‘áº¿n model Ä‘Ã£ train
        
    Returns:
        results dict
    """
    print("=" * 60)
    print("ğŸ“Š ÄÃNH GIÃ HAR MODEL")
    print("=" * 60)
    
    # 1. Load model
    print(f"\nğŸ“‚ BÆ¯á»šC 1: Load model tá»« {model_path}")
    if not os.path.exists(model_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y model táº¡i {model_path}")
        print("HÃ£y cháº¡y: python src/train.py")
        sys.exit(1)
    
    model = keras.models.load_model(model_path)
    print("âœ… ÄÃ£ load model thÃ nh cÃ´ng")
    model.summary()
    
    # 2. Load dá»¯ liá»‡u
    print("\nğŸ“‚ BÆ¯á»šC 2: Load test data")
    X_train, X_test, y_train, y_test, _, _, activity_labels = load_uci_har_data()
    
    # 3. Preprocessing
    print("\nğŸ”§ BÆ¯á»šC 3: Preprocessing")
    X_train_p, X_val_p, X_test_p, y_train_p, y_val_p, y_test_p, scaler = preprocess_data(
        X_train, X_test, y_train, y_test,
        scaler_type='standard',
        validation_split=0.2,
        save_scaler=False
    )
    
    # 4. Reshape
    print("\nğŸ”„ BÆ¯á»šC 4: Reshape dá»¯ liá»‡u")
    _, _, X_test_r = reshape_for_cnn(X_train_p, X_val_p, X_test_p)
    
    # 5. Dá»± Ä‘oÃ¡n
    print("\nğŸ¯ BÆ¯á»šC 5: Dá»± Ä‘oÃ¡n trÃªn test set")
    y_pred_proba = model.predict(X_test_r, verbose=0)
    y_pred = np.argmax(y_pred_proba, axis=1)
    
    # 6. TÃ­nh metrics
    print("\nğŸ“Š BÆ¯á»šC 6: TÃ­nh toÃ¡n metrics")
    accuracy = accuracy_score(y_test_p, y_pred)
    
    print("\n" + "=" * 60)
    print(f"âœ… ACCURACY: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print("=" * 60)
    
    # 7. Classification report
    print("\nğŸ“‹ BÆ¯á»šC 7: Classification Report")
    class_names = [activity_labels[i+1] for i in range(6)]
    report = classification_report(y_test_p, y_pred, target_names=class_names, digits=4)
    print(report)
    
    # 8. Confusion matrix
    print("\nğŸ“Š BÆ¯á»šC 8: Confusion Matrix")
    cm = confusion_matrix(y_test_p, y_pred)
    plot_confusion_matrix(cm, class_names, model_path)
    
    # 9. LÆ°u káº¿t quáº£
    print("\nğŸ’¾ BÆ¯á»šC 9: LÆ°u káº¿t quáº£")
    save_evaluation_results(accuracy, report, cm, model_path)
    
    # 10. PhÃ¢n tÃ­ch lá»—i
    print("\nğŸ” BÆ¯á»šC 10: PhÃ¢n tÃ­ch lá»—i")
    analyze_errors(y_test_p, y_pred, class_names)
    
    print("\n" + "=" * 60)
    print("ğŸ‰ ÄÃNH GIÃ HOÃ€N Táº¤T!")
    print("=" * 60)
    
    results = {
        'accuracy': accuracy,
        'predictions': y_pred,
        'true_labels': y_test_p,
        'confusion_matrix': cm
    }
    
    return results


def plot_confusion_matrix(cm, class_names, model_path):
    """
    Váº½ confusion matrix
    
    Args:
        cm: Confusion matrix
        class_names: TÃªn cÃ¡c classes
        model_path: ÄÆ°á»ng dáº«n model (Ä‘á»ƒ Ä‘áº·t tÃªn file)
    """
    plt.figure(figsize=(10, 8))
    
    # Normalize confusion matrix
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    # Váº½ heatmap
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'Normalized Count'})
    
    plt.title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')
    plt.ylabel('True Label', fontsize=12)
    plt.xlabel('Predicted Label', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    
    # LÆ°u figure
    model_name = os.path.splitext(os.path.basename(model_path))[0]
    plot_path = f'models/confusion_matrix_{model_name}.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"âœ… ÄÃ£ lÆ°u confusion matrix táº¡i: {plot_path}")
    
    plt.close()


def save_evaluation_results(accuracy, report, cm, model_path):
    """
    LÆ°u káº¿t quáº£ evaluation vÃ o file
    
    Args:
        accuracy: Accuracy score
        report: Classification report
        cm: Confusion matrix
        model_path: ÄÆ°á»ng dáº«n model
    """
    model_name = os.path.splitext(os.path.basename(model_path))[0]
    results_path = f'models/evaluation_results_{model_name}.txt'
    
    with open(results_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("Káº¾T QUáº¢ ÄÃNH GIÃ HAR MODEL\n")
        f.write("=" * 60 + "\n\n")
        f.write(f"Model: {model_path}\n")
        f.write(f"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\n\n")
        f.write("=" * 60 + "\n")
        f.write("CLASSIFICATION REPORT\n")
        f.write("=" * 60 + "\n")
        f.write(report)
        f.write("\n\n")
        f.write("=" * 60 + "\n")
        f.write("CONFUSION MATRIX\n")
        f.write("=" * 60 + "\n")
        f.write(str(cm))
        f.write("\n")
    
    print(f"âœ… ÄÃ£ lÆ°u káº¿t quáº£ táº¡i: {results_path}")


def analyze_errors(y_true, y_pred, class_names):
    """
    PhÃ¢n tÃ­ch cÃ¡c lá»—i dá»± Ä‘oÃ¡n
    
    Args:
        y_true: True labels
        y_pred: Predicted labels
        class_names: TÃªn cÃ¡c classes
    """
    errors = y_true != y_pred
    num_errors = np.sum(errors)
    
    print(f"\nğŸ“Š Tá»•ng sá»‘ lá»—i: {num_errors}/{len(y_true)} ({num_errors/len(y_true)*100:.2f}%)")
    
    if num_errors > 0:
        print("\nğŸ” Top 5 cáº·p lá»—i thÆ°á»ng gáº·p:")
        error_pairs = {}
        for true_label, pred_label in zip(y_true[errors], y_pred[errors]):
            pair = (class_names[true_label], class_names[pred_label])
            error_pairs[pair] = error_pairs.get(pair, 0) + 1
        
        sorted_pairs = sorted(error_pairs.items(), key=lambda x: x[1], reverse=True)
        for i, ((true_class, pred_class), count) in enumerate(sorted_pairs[:5], 1):
            print(f"  {i}. {true_class:20s} â†’ {pred_class:20s}: {count:3d} lá»—i")


if __name__ == '__main__':
    # ÄÃ¡nh giÃ¡ model
    MODEL_PATH = 'models/har_model_cnn_simple.h5'
    
    results = evaluate_model(MODEL_PATH)
    
    print("\nâœ… Script hoÃ n táº¥t!")

