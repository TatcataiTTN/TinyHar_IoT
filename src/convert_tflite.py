#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TFLite Conversion Script
Chuyá»ƒn Ä‘á»•i Keras model sang TFLite vÃ  C header file cho ESP32
"""

import os
import sys
import numpy as np
import tensorflow as tf
from tensorflow import keras

def convert_to_tflite(model_path, output_path=None, quantize=True, test_data=None):
    """
    Chuyá»ƒn Ä‘á»•i Keras model sang TFLite
    
    Args:
        model_path: ÄÆ°á»ng dáº«n Keras model (.h5)
        output_path: ÄÆ°á»ng dáº«n output (.tflite)
        quantize: CÃ³ quantize khÃ´ng (int8)
        test_data: Dá»¯ liá»‡u test Ä‘á»ƒ calibrate quantization
        
    Returns:
        tflite_model (bytes)
    """
    print("=" * 60)
    print("ğŸ”„ CHUYá»‚N Äá»”I MODEL SANG TFLITE")
    print("=" * 60)
    
    # 1. Load Keras model
    print(f"\nğŸ“‚ BÆ¯á»šC 1: Load Keras model tá»« {model_path}")
    if not os.path.exists(model_path):
        print(f"âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y model táº¡i {model_path}")
        sys.exit(1)
    
    model = keras.models.load_model(model_path)
    print("âœ… ÄÃ£ load model thÃ nh cÃ´ng")
    
    # KÃ­ch thÆ°á»›c model gá»‘c
    model_size = os.path.getsize(model_path)
    print(f"ğŸ“ KÃ­ch thÆ°á»›c model gá»‘c: {model_size / 1024:.2f} KB")
    
    # 2. Táº¡o converter
    print("\nğŸ”§ BÆ¯á»šC 2: Táº¡o TFLite converter")
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    
    # 3. Quantization
    if quantize:
        print("\nâš™ï¸  BÆ¯á»šC 3: Ãp dá»¥ng quantization (int8)")
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        
        # Náº¿u cÃ³ test data, dÃ¹ng Ä‘á»ƒ calibrate
        if test_data is not None:
            print("ğŸ“Š Sá»­ dá»¥ng representative dataset Ä‘á»ƒ calibrate")
            
            def representative_dataset():
                for i in range(min(100, len(test_data))):
                    yield [test_data[i:i+1].astype(np.float32)]
            
            converter.representative_dataset = representative_dataset
            converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
            converter.inference_input_type = tf.int8
            converter.inference_output_type = tf.int8
        
        print("âœ… ÄÃ£ cáº¥u hÃ¬nh quantization")
    else:
        print("\nâš™ï¸  BÆ¯á»šC 3: KhÃ´ng quantize (giá»¯ float32)")
    
    # 4. Convert
    print("\nğŸ”„ BÆ¯á»šC 4: Äang convert...")
    try:
        tflite_model = converter.convert()
        print("âœ… Convert thÃ nh cÃ´ng!")
    except Exception as e:
        print(f"âŒ Lá»—i khi convert: {e}")
        sys.exit(1)
    
    # 5. LÆ°u TFLite model
    if output_path is None:
        model_name = os.path.splitext(os.path.basename(model_path))[0]
        output_path = f'models/{model_name}.tflite'
    
    print(f"\nğŸ’¾ BÆ¯á»šC 5: LÆ°u TFLite model táº¡i {output_path}")
    with open(output_path, 'wb') as f:
        f.write(tflite_model)
    
    tflite_size = len(tflite_model)
    print(f"âœ… ÄÃ£ lÆ°u TFLite model")
    print(f"ğŸ“ KÃ­ch thÆ°á»›c TFLite: {tflite_size / 1024:.2f} KB")
    print(f"ğŸ“‰ Giáº£m: {(1 - tflite_size/model_size) * 100:.1f}%")
    
    # 6. Kiá»ƒm tra model
    print("\nğŸ§ª BÆ¯á»šC 6: Kiá»ƒm tra TFLite model")
    verify_tflite_model(output_path, test_data)
    
    print("\n" + "=" * 60)
    print("âœ… CHUYá»‚N Äá»”I HOÃ€N Táº¤T!")
    print("=" * 60)
    
    return tflite_model


def verify_tflite_model(tflite_path, test_data=None):
    """
    Kiá»ƒm tra TFLite model hoáº¡t Ä‘á»™ng Ä‘Ãºng
    
    Args:
        tflite_path: ÄÆ°á»ng dáº«n TFLite model
        test_data: Dá»¯ liá»‡u test
    """
    # Load TFLite model
    interpreter = tf.lite.Interpreter(model_path=tflite_path)
    interpreter.allocate_tensors()
    
    # Láº¥y input/output details
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    print(f"âœ… Input shape: {input_details[0]['shape']}")
    print(f"âœ… Input dtype: {input_details[0]['dtype']}")
    print(f"âœ… Output shape: {output_details[0]['shape']}")
    print(f"âœ… Output dtype: {output_details[0]['dtype']}")
    
    # Test inference náº¿u cÃ³ test data
    if test_data is not None and len(test_data) > 0:
        print("\nğŸ§ª Test inference vá»›i 1 sample...")
        
        # Chuáº©n bá»‹ input
        test_sample = test_data[0:1].astype(input_details[0]['dtype'])
        interpreter.set_tensor(input_details[0]['index'], test_sample)
        
        # Run inference
        interpreter.invoke()
        
        # Láº¥y output
        output = interpreter.get_tensor(output_details[0]['index'])
        predicted_class = np.argmax(output[0])
        
        print(f"âœ… Inference thÃ nh cÃ´ng!")
        print(f"âœ… Predicted class: {predicted_class}")
        print(f"âœ… Confidence: {output[0][predicted_class]:.4f}")


def convert_to_c_header(tflite_path, output_path=None):
    """
    Chuyá»ƒn Ä‘á»•i TFLite model sang C header file cho ESP32
    
    Args:
        tflite_path: ÄÆ°á»ng dáº«n TFLite model
        output_path: ÄÆ°á»ng dáº«n output (.h)
        
    Returns:
        header_content (string)
    """
    print("\n" + "=" * 60)
    print("ğŸ”„ CHUYá»‚N Äá»”I SANG C HEADER FILE")
    print("=" * 60)
    
    # Äá»c TFLite model
    with open(tflite_path, 'rb') as f:
        tflite_model = f.read()
    
    model_size = len(tflite_model)
    print(f"ğŸ“ KÃ­ch thÆ°á»›c model: {model_size} bytes ({model_size/1024:.2f} KB)")
    
    # Táº¡o tÃªn biáº¿n
    model_name = os.path.splitext(os.path.basename(tflite_path))[0]
    var_name = model_name.replace('-', '_').replace('.', '_')
    
    # Táº¡o C header content
    header_content = f"""// Auto-generated C header file for TFLite model
// Model: {model_name}
// Size: {model_size} bytes ({model_size/1024:.2f} KB)
// Generated: {__import__('datetime').datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

#ifndef {var_name.upper()}_H
#define {var_name.upper()}_H

const unsigned int {var_name}_len = {model_size};
const unsigned char {var_name}_data[] = {{
"""
    
    # ThÃªm dá»¯ liá»‡u model (16 bytes per line)
    for i in range(0, model_size, 16):
        line = "  "
        for j in range(16):
            if i + j < model_size:
                line += f"0x{tflite_model[i+j]:02x}, "
        header_content += line + "\n"
    
    header_content += "};\n\n#endif  // " + var_name.upper() + "_H\n"
    
    # LÆ°u header file
    if output_path is None:
        output_path = f'models/{model_name}.h'
    
    with open(output_path, 'w') as f:
        f.write(header_content)
    
    print(f"âœ… ÄÃ£ lÆ°u C header táº¡i: {output_path}")
    print(f"ğŸ“ TÃªn biáº¿n: {var_name}_data")
    print(f"ğŸ“ KÃ­ch thÆ°á»›c: {var_name}_len")
    
    print("\nğŸ’¡ CÃ¡ch sá»­ dá»¥ng trong ESP32:")
    print(f'   #include "{os.path.basename(output_path)}"')
    print(f'   const tflite::Model* model = tflite::GetModel({var_name}_data);')
    
    return header_content


if __name__ == '__main__':
    # Cáº¥u hÃ¬nh
    MODEL_PATH = 'models/har_model_cnn_simple.h5'
    QUANTIZE = True
    
    # Load test data Ä‘á»ƒ calibrate quantization
    print("ğŸ“‚ Load test data Ä‘á»ƒ calibrate quantization...")
    from data_loader import load_uci_har_data
    from preprocessing import preprocess_data, reshape_for_cnn
    
    X_train, X_test, y_train, y_test, _, _, _ = load_uci_har_data()
    X_train_p, X_val_p, X_test_p, _, _, _, _ = preprocess_data(
        X_train, X_test, y_train, y_test,
        save_scaler=False
    )
    _, _, X_test_r = reshape_for_cnn(X_train_p, X_val_p, X_test_p)
    
    # Convert sang TFLite
    tflite_model = convert_to_tflite(
        MODEL_PATH,
        quantize=QUANTIZE,
        test_data=X_test_r
    )
    
    # Convert sang C header
    model_name = os.path.splitext(os.path.basename(MODEL_PATH))[0]
    tflite_path = f'models/{model_name}.tflite'
    convert_to_c_header(tflite_path)
    
    print("\nâœ… Script hoÃ n táº¥t!")

