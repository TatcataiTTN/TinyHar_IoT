# B√ÅO C√ÅO SO S√ÅNH C√ÅC MODELS NH·∫¨N DI·ªÜN HO·∫†T ƒê·ªòNG (HAR)

## üìä T·ªîNG QUAN K·∫æT QU·∫¢ TRAINING

Ng√†y t·∫°o: 2026-01-14
T·ªïng s·ªë models: 6
Dataset: UCI HAR Dataset (7,352 training samples, 2,947 test samples)

---

## üìà B·∫¢NG K·∫æT QU·∫¢ CHI TI·∫æT

| STT | Model | ƒê·ªô Ch√≠nh X√°c | Loss | S·ªë Parameters | K√≠ch Th∆∞·ªõc | Th·ªùi Gian Training |
|-----|-------|--------------|------|---------------|------------|-------------------|
| 1 | CNN ƒê∆°n Gi·∫£n | **95.89%** | 0.1462 | 283,718 | 1.08 MB | 42.4s |
| 2 | CNN S√¢u | **92.06%** | 0.2806 | 53,382 | 0.20 MB | 170.9s |
| 3 | LSTM | 82.97% | 0.5092 | 31,814 | 0.12 MB | 1,166.1s |
| 4 | CNN-LSTM | **89.18%** | 0.2821 | 41,638 | 0.16 MB | 261.5s |
| 5 | Depthwise CNN | 81.71% | 0.4827 | 29,520 | 0.11 MB | 137.8s |
| 6 | CNN Attention | **86.83%** | 0.4635 | 31,814 | 0.12 MB | 221.2s |

---

## ü•á PH√ÇN T√çCH CHI TI·∫æT

### 1. MODEL C√ì HI·ªÜU SU·∫§T T·ªêT NH·∫§T: CNN ƒê∆†N GI·∫¢N

**K·∫øt qu·∫£:**
- ƒê·ªô ch√≠nh x√°c: **95.89%** (cao nh·∫•t)
- Test loss: 0.1462 (th·∫•p nh·∫•t - t·ªët nh·∫•t)
- Th·ªùi gian training: 42.4 gi√¢y (nhanh th·ª© 2)

**T·∫°i sao CNN ƒê∆°n Gi·∫£n ho·∫°t ƒë·ªông t·ªët nh·∫•t?**

1. **Ki·∫øn tr√∫c ph√π h·ª£p v·ªõi d·ªØ li·ªáu:**
   - UCI HAR dataset c√≥ 561 features ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω s·∫µn (time-domain v√† frequency-domain)
   - CNN ƒë∆°n gi·∫£n v·ªõi c√°c conv layers c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c c√°c patterns t·ª´ features n√†y r·∫•t hi·ªáu qu·∫£
   - Kh√¥ng c·∫ßn ki·∫øn tr√∫c qu√° ph·ª©c t·∫°p v√¨ features ƒë√£ ƒë∆∞·ª£c engineering t·ªët

2. **S·ªë l∆∞·ª£ng parameters h·ª£p l√Ω:**
   - 283,718 parameters - ƒë·ªß l·ªõn ƒë·ªÉ h·ªçc ƒë∆∞·ª£c c√°c patterns ph·ª©c t·∫°p
   - Kh√¥ng qu√° l·ªõn n√™n tr√°nh ƒë∆∞·ª£c overfitting
   - Validation accuracy cao (99.11%) cho th·∫•y model generalize t·ªët

3. **Training ·ªïn ƒë·ªãnh:**
   - Loss gi·∫£m ƒë·ªÅu ƒë·∫∑n qua c√°c epochs
   - Kh√¥ng c√≥ d·∫•u hi·ªáu overfitting nghi√™m tr·ªçng
   - Early stopping v√† ReduceLROnPlateau gi√∫p t·ªëi ∆∞u h√≥a t·ªët

**Nh∆∞·ª£c ƒëi·ªÉm:**
- K√≠ch th∆∞·ªõc l·ªõn nh·∫•t (1.08 MB) - c√≥ th·ªÉ kh√≥ deploy l√™n ESP32 v·ªõi b·ªô nh·ªõ h·∫°n ch·∫ø
- C·∫ßn quantization ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc xu·ªëng c√≤n ~270 KB

---

### 2. MODEL HI·ªÜU QU·∫¢ NH·∫§T: CNN S√ÇU

**K·∫øt qu·∫£:**
- ƒê·ªô ch√≠nh x√°c: **92.06%** (cao th·ª© 2)
- K√≠ch th∆∞·ªõc: **0.20 MB** (nh·ªè th·ª© 3)
- T·ª∑ l·ªá accuracy/size: **460.3 %/MB** (t·ªët nh·∫•t)

**T·∫°i sao CNN S√¢u l√† model hi·ªáu qu·∫£ nh·∫•t?**

1. **Trade-off tuy·ªát v·ªùi:**
   - Accuracy ch·ªâ k√©m CNN ƒê∆°n Gi·∫£n 3.83%
   - Nh∆∞ng k√≠ch th∆∞·ªõc nh·ªè h∆°n **5.4 l·∫ßn** (1.08 MB vs 0.20 MB)
   - Ch·ªâ c√≥ 53,382 parameters - r·∫•t ph√π h·ª£p cho embedded systems

2. **Ki·∫øn tr√∫c t·ªëi ∆∞u:**
   - S·ª≠ d·ª•ng nhi·ªÅu conv layers v·ªõi BatchNormalization
   - Gi·∫£m s·ªë filters ·ªü m·ªói layer ƒë·ªÉ gi·∫£m parameters
   - V·∫´n ƒë·ªß s√¢u ƒë·ªÉ h·ªçc ƒë∆∞·ª£c features ph·ª©c t·∫°p

3. **Ph√π h·ª£p cho ESP32:**
   - K√≠ch th∆∞·ªõc 0.20 MB c√≥ th·ªÉ gi·∫£m xu·ªëng ~50 KB sau quantization
   - Inference time nhanh do √≠t parameters
   - ƒê·ªô ch√≠nh x√°c 92% v·∫´n r·∫•t t·ªët cho ·ª©ng d·ª•ng th·ª±c t·∫ø

**Khuy·∫øn ngh·ªã:** ‚≠ê **ƒê√ÇY L√Ä MODEL T·ªêT NH·∫§T ƒê·ªÇ DEPLOY L√äN ESP32**

---

### 3. PH√ÇN T√çCH C√ÅC MODELS KH√ÅC

#### 3.1. LSTM (82.97%)

**T·∫°i sao LSTM ho·∫°t ƒë·ªông k√©m?**

1. **Kh√¥ng ph√π h·ª£p v·ªõi d·ªØ li·ªáu:**
   - UCI HAR features ƒë√£ ƒë∆∞·ª£c aggregate (mean, std, max, min) t·ª´ time windows
   - Kh√¥ng c√≤n temporal dependencies r√µ r√†ng
   - LSTM c·∫ßn raw time-series data ƒë·ªÉ ph√°t huy t·ªëi ƒëa

2. **Training ch·∫≠m:**
   - 1,166 gi√¢y (19.4 ph√∫t) - ch·∫≠m nh·∫•t
   - LSTM c√≥ nhi·ªÅu operations tu·∫ßn t·ª±, kh√≥ parallelize
   - Kh√¥ng hi·ªáu qu·∫£ cho training

3. **Overfitting:**
   - Validation accuracy (89.67%) cao h∆°n test accuracy (82.97%)
   - Ch√™nh l·ªách 6.7% cho th·∫•y model kh√¥ng generalize t·ªët

**K·∫øt lu·∫≠n:** LSTM kh√¥ng ph·∫£i l·ª±a ch·ªçn t·ªët cho UCI HAR dataset v·ªõi features ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω

---

#### 3.2. CNN-LSTM (89.18%)

**Ph√¢n t√≠ch:**

1. **K·∫øt h·ª£p hai ki·∫øn tr√∫c:**
   - CNN extract spatial features
   - LSTM h·ªçc temporal dependencies
   - Accuracy 89.18% - kh√° t·ªët nh∆∞ng kh√¥ng xu·∫•t s·∫Øc

2. **V·∫•n ƒë·ªÅ:**
   - K√≠ch th∆∞·ªõc 0.16 MB - trung b√¨nh
   - Training time 261.5s - kh√° l√¢u
   - Ph·ª©c t·∫°p h∆°n nh∆∞ng kh√¥ng c·∫£i thi·ªán nhi·ªÅu so v·ªõi CNN thu·∫ßn

3. **Trade-off kh√¥ng t·ªët:**
   - Accuracy ch·ªâ cao h∆°n CNN S√¢u 2.88%
   - Nh∆∞ng training ch·∫≠m h∆°n 1.5 l·∫ßn
   - K√≠ch th∆∞·ªõc l·ªõn h∆°n 1.25 l·∫ßn

**K·∫øt lu·∫≠n:** CNN-LSTM kh√¥ng mang l·∫°i l·ª£i √≠ch ƒë√°ng k·ªÉ so v·ªõi CNN thu·∫ßn

---

#### 3.3. Depthwise CNN (81.71%)

**T·∫°i sao Depthwise CNN c√≥ accuracy th·∫•p nh·∫•t?**

1. **Ki·∫øn tr√∫c qu√° ƒë∆°n gi·∫£n:**
   - Ch·ªâ c√≥ 29,520 parameters - √≠t nh·∫•t trong t·∫•t c·∫£ models
   - Depthwise separable convolutions gi·∫£m parameters qu√° nhi·ªÅu
   - Kh√¥ng ƒë·ªß capacity ƒë·ªÉ h·ªçc c√°c patterns ph·ª©c t·∫°p

2. **C·∫£i thi·ªán ƒë√£ th·ª±c hi·ªán:**
   - ƒê√£ th√™m BatchNormalization
   - ƒê√£ th√™m block th·ª© 3 v·ªõi 128 filters
   - ƒê√£ tƒÉng Dense layer t·ª´ 64 ‚Üí 128 units
   - Nh∆∞ng v·∫´n ch∆∞a ƒë·ªß ƒë·ªÉ ƒë·∫°t accuracy cao

3. **∆Øu ƒëi·ªÉm:**
   - K√≠ch th∆∞·ªõc nh·ªè nh·∫•t: 0.11 MB (~28 KB sau quantization)
   - Training nhanh: 137.8s
   - R·∫•t ph√π h·ª£p cho devices c√≥ b·ªô nh·ªõ c·ª±c k·ª≥ h·∫°n ch·∫ø

**K·∫øt lu·∫≠n:** Depthwise CNN ph√π h·ª£p khi c·∫ßn model c·ª±c k·ª≥ nh·ªè g·ªçn, ch·∫•p nh·∫≠n accuracy th·∫•p h∆°n

---

#### 3.4. CNN Attention (86.83%)

**Ph√¢n t√≠ch:**

1. **Attention mechanism:**
   - Gi√∫p model focus v√†o c√°c features quan tr·ªçng
   - Accuracy 86.83% - kh√° t·ªët
   - K√≠ch th∆∞·ªõc 0.12 MB - nh·ªè

2. **V·∫•n ƒë·ªÅ:**
   - Kh√¥ng c·∫£i thi·ªán nhi·ªÅu so v·ªõi CNN thu·∫ßn
   - Training time 221.2s - kh√° l√¢u
   - Attention layer tƒÉng complexity nh∆∞ng kh√¥ng tƒÉng accuracy ƒë√°ng k·ªÉ

3. **Nguy√™n nh√¢n:**
   - UCI HAR features ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω t·ªët
   - Kh√¥ng c·∫ßn attention ƒë·ªÉ select features
   - Simple CNN ƒë√£ ƒë·ªß hi·ªáu qu·∫£

**K·∫øt lu·∫≠n:** Attention kh√¥ng mang l·∫°i l·ª£i √≠ch r√µ r·ªát cho b√†i to√°n n√†y

---

## üéØ KHUY·∫æN NGH·ªä DEPLOY L√äN ESP32

### L·ª±a ch·ªçn 1: CNN S√ÇU (KHUY·∫æN NGH·ªä) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**L√Ω do:**
- ‚úÖ Accuracy cao: 92.06%
- ‚úÖ K√≠ch th∆∞·ªõc nh·ªè: 0.20 MB ‚Üí ~50 KB sau quantization
- ‚úÖ Trade-off t·ªët nh·∫•t gi·ªØa accuracy v√† size
- ‚úÖ Inference nhanh do √≠t parameters
- ‚úÖ Ph√π h·ª£p v·ªõi ESP32 (520 KB SRAM, 4 MB Flash)

**·ª®ng d·ª•ng:** Ph√π h·ª£p cho h·∫ßu h·∫øt c√°c ·ª©ng d·ª•ng HAR tr√™n ESP32

---

### L·ª±a ch·ªçn 2: CNN ƒê∆†N GI·∫¢N (N·∫øu c·∫ßn accuracy cao nh·∫•t) ‚≠ê‚≠ê‚≠ê‚≠ê

**L√Ω do:**
- ‚úÖ Accuracy cao nh·∫•t: 95.89%
- ‚ö†Ô∏è K√≠ch th∆∞·ªõc l·ªõn: 1.08 MB ‚Üí ~270 KB sau quantization
- ‚ö†Ô∏è C·∫ßn ESP32 v·ªõi Flash l·ªõn (4 MB tr·ªü l√™n)

**·ª®ng d·ª•ng:** Khi accuracy l√† ∆∞u ti√™n s·ªë 1 v√† ESP32 c√≥ ƒë·ªß b·ªô nh·ªõ

---

### L·ª±a ch·ªçn 3: DEPTHWISE CNN (N·∫øu b·ªô nh·ªõ c·ª±c k·ª≥ h·∫°n ch·∫ø) ‚≠ê‚≠ê‚≠ê

**L√Ω do:**
- ‚úÖ K√≠ch th∆∞·ªõc nh·ªè nh·∫•t: 0.11 MB ‚Üí ~28 KB sau quantization
- ‚ö†Ô∏è Accuracy th·∫•p: 81.71%
- ‚úÖ Ph√π h·ª£p cho ESP32 v·ªõi Flash nh·ªè (2 MB)

**·ª®ng d·ª•ng:** Khi b·ªô nh·ªõ l√† gi·ªõi h·∫°n ch√≠nh, ch·∫•p nh·∫≠n accuracy th·∫•p h∆°n

---

## üìä SO S√ÅNH K√çCH TH∆Ø·ªöC SAU QUANTIZATION (D·ª∞ KI·∫æN)

| Model | K√≠ch Th∆∞·ªõc G·ªëc | Sau Quantization (int8) | Gi·∫£m |
|-------|----------------|-------------------------|------|
| CNN ƒê∆°n Gi·∫£n | 1.08 MB | ~270 KB | 75% |
| CNN S√¢u | 0.20 MB | ~50 KB | 75% |
| LSTM | 0.12 MB | ~30 KB | 75% |
| CNN-LSTM | 0.16 MB | ~40 KB | 75% |
| Depthwise CNN | 0.11 MB | ~28 KB | 75% |
| CNN Attention | 0.12 MB | ~30 KB | 75% |

---

## üéì K·∫æT LU·∫¨N T·ªîNG QUAN

### Models Xu·∫•t S·∫Øc (‚â•90%):
1. **CNN ƒê∆°n Gi·∫£n: 95.89%** - T·ªët nh·∫•t v·ªÅ accuracy
2. **CNN S√¢u: 92.06%** - T·ªët nh·∫•t v·ªÅ trade-off

### Models T·ªët (85-90%):
3. **CNN-LSTM: 89.18%** - T·ªët nh∆∞ng ph·ª©c t·∫°p kh√¥ng c·∫ßn thi·∫øt
4. **CNN Attention: 86.83%** - Attention kh√¥ng mang l·∫°i l·ª£i √≠ch r√µ r·ªát

### Models C·∫ßn C·∫£i Thi·ªán (<85%):
5. **LSTM: 82.97%** - Kh√¥ng ph√π h·ª£p v·ªõi d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
6. **Depthwise CNN: 81.71%** - Qu√° ƒë∆°n gi·∫£n, c·∫ßn th√™m capacity

---

## üí° KHUY·∫æN NGH·ªä CU·ªêI C√ôNG

**Cho ESP32 v·ªõi 4 MB Flash:**
‚Üí S·ª≠ d·ª•ng **CNN S√ÇU** (92.06% accuracy, ~50 KB sau quantization)

**Cho ESP32 v·ªõi 2 MB Flash:**
‚Üí S·ª≠ d·ª•ng **Depthwise CNN** (81.71% accuracy, ~28 KB sau quantization)

**Cho ·ª©ng d·ª•ng c·∫ßn accuracy cao nh·∫•t:**
‚Üí S·ª≠ d·ª•ng **CNN ƒê∆°n Gi·∫£n** (95.89% accuracy, ~270 KB sau quantization)

---

**T√°c gi·∫£:** AI Training System
**Ng√†y:** 2026-01-14
**Version:** 1.0

